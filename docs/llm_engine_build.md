# LLM Engine Build Workflow

This document captures the governed workflow for compiling the Qwen3-32B model into a TensorRT-LLM engine with W4A8 quantisation, paged KV cache, and FP8 context FMHA. All parameters are pinned to guarantee reproducibility.

## 1. Environment & Dependencies

| Item | Value |
| --- | --- |
| GPU | NVIDIA RTX 5090 (32 GB GDDR7) |
| Driver | `555.85` |
| Build container | `nvcr.io/nvidia/tensorrt-llm/release:24.05-trtllm-python-py3` |
| Triton container | `nvcr.io/nvidia/tritonserver:24.05-trtllm-python-py3` |
| CUDA version | `12.4` |
| TensorRT version | `9.0` |
| Volumes | `/srv/artifacts`, `/srv/cache` |
| Env vars | `CUDA_VISIBLE_DEVICES=0`, `HF_HOME=/srv/cache/hf` |

Launch command (generated by `ContainerLaunchPlan.docker_args()`):

```bash
docker run --rm --gpus all \
  -e CUDA_VISIBLE_DEVICES=0 \
  -e HF_HOME=/srv/cache/hf \
  -v /srv/artifacts:/srv/artifacts:rw \
  -v /srv/cache:/srv/cache:rw \
  nvcr.io/nvidia/tensorrt-llm/release:24.05-trtllm-python-py3 \
  bash -lc scripts/llm/build_engine.sh
```

## 2. Model Snapshot

- Model ID: `Qwen/Qwen3-32B`
- Revision hash is recorded via `tools/llm/download_snapshot.py`
- Tokenizer files stored in `/srv/artifacts/models/qwen3/tokenizer`
- License acceptance documented in `LICENSE_ACCEPTED.txt`

## 3. Engine Build Pipeline

1. Download snapshot: `python tools/llm/download_snapshot.py --model-id Qwen/Qwen3-32B --revision main --output /srv/artifacts/models/qwen3`
2. Prepare checkpoint: `python tools/llm/prepare_qwen3.py --input /srv/artifacts/models/qwen3 --output /srv/artifacts/workspace/checkpoints`
3. Quantise with AWQ W4A8: `python tools/llm/quantize_awq.py --input ... --output ... --calibration ...`
4. Build engine:

   ```bash
   trtllm-build \
     --checkpoint-dir /srv/artifacts/workspace/quantized \
     --output-dir /srv/artifacts/engines \
     --use_paged_context_fmha enable \
     --kv_cache_type paged \
     --kv_cache_precision fp8 \
     --tokens_per_block 32 \
     --max_batch_size 4 \
     --max_input_tokens 4096 \
     --max_output_tokens 1024
   ```

5. Write manifest: `python tools/llm/write_manifest.py --workspace /srv/artifacts/workspace --engine /srv/artifacts/engines/qwen3-32b-w4a8.plan --output /srv/artifacts/manifests/qwen3-32b.json`

## 4. Validation & Handoff

- Smoke test: `python tools/llm/smoke_test.py --engine /srv/artifacts/engines/qwen3-32b-w4a8.plan --output /srv/artifacts/smoke/qwen3.json`
- Manifest fields include: engine hash (`sha256`), build flags, calibration dataset hash
- Artifacts pushed to governance registry via `dd publish --channel engine-qwen3`

## 5. Build Script

`scripts/llm/build_engine.sh` orchestrates the entire flow:

```bash
#!/usr/bin/env bash
set -euo pipefail

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "${SCRIPT_DIR}/../.." && pwd)"
WORKSPACE="${PROJECT_ROOT}/workspace"
MODEL_DIR="${WORKSPACE}/models/qwen3-32b"
ENGINE_DIR="${WORKSPACE}/engines"
MANIFEST_DIR="${WORKSPACE}/manifests"
CALIB_DIR="${WORKSPACE}/calibration"

mkdir -p "${MODEL_DIR}" "${ENGINE_DIR}" "${MANIFEST_DIR}" "${CALIB_DIR}"

python3 ${PROJECT_ROOT}/tools/llm/download_snapshot.py --model-id Qwen/Qwen3-32B --revision main --output "${MODEL_DIR}"
python3 ${PROJECT_ROOT}/tools/llm/prepare_qwen3.py --input "${MODEL_DIR}" --output "${WORKSPACE}/checkpoints"
python3 ${PROJECT_ROOT}/tools/llm/quantize_awq.py --input "${WORKSPACE}/checkpoints" --output "${WORKSPACE}/quantized" --calibration "${CALIB_DIR}"

trtllm-build \
  --checkpoint-dir "${WORKSPACE}/quantized" \
  --output-dir "${ENGINE_DIR}" \
  --use_paged_context_fmha enable \
  --kv_cache_type paged \
  --kv_cache_precision fp8 \
  --tokens_per_block 32 \
  --max_batch_size 4 \
  --max_input_tokens 4096 \
  --max_output_tokens 1024

python3 ${PROJECT_ROOT}/tools/llm/write_manifest.py --workspace "${WORKSPACE}" --engine "${ENGINE_DIR}/qwen3-32b-w4a8.plan" --output "${MANIFEST_DIR}/qwen3-32b.json"

python3 ${PROJECT_ROOT}/tools/llm/smoke_test.py --engine "${ENGINE_DIR}/qwen3-32b-w4a8.plan" --output "${WORKSPACE}/smoke/qwen3.json"
```
